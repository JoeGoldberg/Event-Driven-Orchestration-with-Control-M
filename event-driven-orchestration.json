{
  "jog-event-driven-orchestration" : {
    "Type" : "Folder",
    "ControlmServer" : "smprod",
    "Description" : "There are two jobs in this folder that are related. The producer generates event pairs that are consumed by the consumer and converted into Control-M jobs using a jinja template stored in an S3 bucket",
    "OrderMethod" : "Manual",
    "Application" : "event orchestration",
    "SubApplication" : "consumer",
    "jog-consume-kafka-events" : {
      "Type" : "Job:EmbeddedScript",
      "Description" : "This embedded script is also available in the Github repository as ctm-kafka-multi-message-consumer.py - note the first line of the script specifies -u which means messages are unbuffered and are written to stdout enabling Control-M to display the output while the job is running",
      "Script" : "#!/usr/bin/python3.11 -u\\n\\nimport argparse\\nimport json\\nimport boto3\\nfrom kafka import KafkaConsumer\\nfrom jinja2 import Template\\nimport requests\\nfrom datetime import datetime\\nimport threading\\nimport io\\n\\nverify_certs = True\\n#   For debugging\\n#    import pdb;pdb.set_trace()\\n\\ndef parse_arguments():\\n    parser = argparse.ArgumentParser(description=\"Consume Kafka messages and create Control-M jobs\")\\n    parser.add_argument(\"-b\", \"--bucket\", required=False, default=\"623469066856-ctmprod-templates\",help=\"S3 bucket name (also used as Secrets Manager secret name)\")\\n    parser.add_argument(\"-s\", \"--ctm-server\", required=False, default=\"smprod\",help=\"Control-M server name\")\\n    parser.add_argument(\"-r\", \"--retention-days\", type=int, required=False, default=14, help=\"Number of days to retain jobs\")\\n    parser.add_argument(\"-n\", \"--num-pairs\", type=int, required=False, default=2, help=\"Number of event pairs to process - not implemented yet\")\\n    return parser.parse_args()\\n\\ndef get_s3_object(bucket, key):\\n    s3 = boto3.client('s3', region_name='us-west-2')\\n    response = s3.get_object(Bucket=bucket, Key=key)\\n    return response['Body'].read().decode('utf-8')\\n\\ndef get_template_from_s3(bucket, key):\\n    return get_s3_object(bucket, key)\\n\\ndef get_ctm_constants(secret_name):\\n    session = boto3.session.Session()\\n    client = session.client(service_name='secretsmanager', region_name='us-west-2')\\n    \\n    try:\\n        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\\n    except Exception as e:\\n        raise e\\n    else:\\n        if 'SecretString' in get_secret_value_response:\\n            secret = get_secret_value_response['SecretString']\\n            return json.loads(secret)\\n        else:\\n            raise ValueError(\"Secret not found or not in the expected format\")\\n\\ndef check_job_exists(job_name, ctm_constants):\\n    headers = {'x-api-key': ctm_constants['CTM_API_KEY']}\\n    jobs_url = f\"{ctm_constants['CTM_API_BASE_URL']}/run/jobs/status\"\\n    today_date = datetime.today().strftime('%y%m%d')\\n    jobs_search = {\"jobname\": job_name, \"orderFromDate\" : today_date}\\n    response = requests.get(jobs_url, headers=headers, params=jobs_search, verify=verify_certs)\\n\\n    response.raise_for_status()\\n    jobs = json.loads(response.text)\\n    num_jobs= int(jobs['returned'])\\n    return num_jobs\\n\\ndef submit_job_to_controlm(json_payload, ctm_constants):\\n    headers = {'x-api-key': ctm_constants['CTM_API_KEY']}\\n    run_url = f\"{ctm_constants['CTM_API_BASE_URL']}/run\"\\n    \\n    # Convert JSON payload to file-like object\\n    job_json_stream = io.StringIO(json_payload)\\n    files = {'jobDefinitionsFile': ('jobs.json', job_json_stream, 'application/json')}\\n    \\n    response = requests.post(run_url, files=files, headers=headers, verify=verify_certs)\\n    response.raise_for_status()\\n\\n    return response.json()\\n\\ndef process_parent_event(message, args, ctm_constants):\\n    parent_id = message['id']\\n    job_name = f\"prnt-{parent_id}\"\\n    now = datetime.now()\\n    current_time = now.strftime(\"%H:%M:%S\")\\n    print(current_time + \" \" + \"Processing parent message \" + parent_id)\\n\\n#    import pdb;pdb.set_trace()\\n\\n    if check_job_exists(job_name, ctm_constants):\\n        raise Exception(f\"Job {job_name} already exists for the current day\")\\n\\n    template_str = get_template_from_s3(args.bucket, 'parents/parent_template.json')\\n    template = Template(template_str)\\n    rendered_payload = json.loads(template.render(\\n        server_name=args.ctm_server,\\n        retention_days=args.retention_days,\\n        job_name=job_name,\\n        parentid=parent_id,\\n        ctmsla=\"23:59\"\\n    ))\\n\\n    json_payload = json.dumps(rendered_payload)\\n\\n    now = datetime.now()\\n    current_time = now.strftime(\"%H:%M:%S\")\\n    print(current_time + \" \" + \"Submitting workflow for parent message \" + parent_id)\\n\\n    return submit_job_to_controlm(json_payload, ctm_constants)\\n\\ndef process_child_event(message, args, ctm_constants):\\n    parent_id = message['parent_id']\\n    parent_job_name = f\"prnt-{parent_id}\"\\n    child_job_name = f\"child-{parent_id}\"\\n    now = datetime.now()\\n    current_time = now.strftime(\"%H:%M:%S\")\\n    print(current_time + \" \" + \"Processing child message for parent \" + parent_id)\\n\\n#    import pdb;pdb.set_trace()\\n\\n    if not check_job_exists(parent_job_name, ctm_constants):\\n        process_no_parent(parent_id, args, ctm_constants)\\n\\n    template_str = get_template_from_s3(args.bucket, 'children/child_template.json')\\n    template = Template(template_str)\\n    rendered_payload = json.loads(template.render(\\n        server_name=args.ctm_server,\\n        retention_days=args.retention_days,\\n        job_name=child_job_name,\\n        parentid=parent_id,\\n        parent_job_name=parent_job_name\\n    ))\\n\\n    json_payload = json.dumps(rendered_payload)\\n\\n    now = datetime.now()\\n    current_time = now.strftime(\"%H:%M:%S\")\\n    print(current_time + \" \" + \"Submitting workflow for child message \" + parent_id)\\n\\n    return submit_job_to_controlm(json_payload, ctm_constants)\\n\\ndef process_no_parent(parent_id, args, ctm_constants):\\n    job_name = f\"prnt-{parent_id}\"\\n    template_str = get_template_from_s3(args.bucket, 'parents/parent_template.json')\\n    template = Template(template_str)\\n    rendered_payload = json.loads(template.render(\\n        server_name=args.ctm_server,\\n        retention_days=args.retention_days,\\n        job_name=job_name,\\n        parentid=parent_id,\\n        ctmsla=\"23:59\"\\n\\n    ))\\n\\n    json_payload = json.dumps(rendered_payload)\\n\\n    return submit_job_to_controlm(json_payload, ctm_constants)\\n\\ndef process_topic(topic_name, process_func, args, ctm_constants):\\n    consumer = KafkaConsumer(topic_name, bootstrap_servers=['localhost:9092'])\\n    for message in consumer:\\n        try:\\n            event = json.loads(message.value)\\n            process_func(event, args, ctm_constants)\\n        except Exception as e:\\n            print(f\"Error processing {topic_name} event: {e}\")\\n\\ndef main():\\n    args = parse_arguments()\\n    ctm_constants = get_ctm_constants(args.bucket)  # Using bucket name as secret name\\n\\n    print(\"Listening for parents and children\")\\n\\n    parent_thread = threading.Thread(target=process_topic, args=('ctm-parent-events', process_parent_event, args, ctm_constants))\\n    children_thread = threading.Thread(target=process_topic, args=('ctm-children-events', process_child_event, args, ctm_constants))\\n\\n    parent_thread.start()\\n    children_thread.start()\\n\\n    parent_thread.join()\\n    children_thread.join()\\n\\nif __name__ == \"__main__\":\\n    main()\\n\\n",
      "FileName" : "kafka-producer.py",
      "Host" : "kafka",
      "Description" : "This job runs forever. Use \"kill\" to terminate it.",
      "RunAs" : "kafka",
      "Application" : "event orchestration"
    },

    "jog-delay-producer": {
      "Type": "Job:EmbeddedScript",
      "Script": "#!/bin/bash\\nsleep $1\\n",
      "FileName": "delaysleep.sh",
      "Variables": [{"PARM1": "10"}],
      "Host" : "kafka",
      "Description" : "Ensure consumer is running before producer",
      "RunAs" : "kafka",
      "Application" : "event orchestration"
    },

    "jog-produce-kafka-events" : {
      "Type" : "Job:EmbeddedScript",
      "Description" : "This embedded script is also available in the Github repository as ctm-kafka-paired-producer.py - note the first line of the script specifies -u which means messages are unbuffered and are written to stdout enabling Control-M to display the output while the job is running",
      "Script" : "#!/usr/bin/python3.11 -u\\n\\nfrom kafka import KafkaProducer\\nimport json\\nimport uuid\\nimport time\\nimport random\\nimport datetime\\nimport argparse\\n\\ndef create_producer(bootstrap_servers):\\n    \"\"\"Create and return a Kafka producer instance.\"\"\"\\n    return KafkaProducer(\\n        bootstrap_servers=bootstrap_servers,\\n        value_serializer=lambda v: json.dumps(v).encode('utf-8')\\n    )\\n\\ndef create_parent_message():\\n    \"\"\"Create a parent message with a unique ID.\"\"\"\\n    return {\\n        'id': str(uuid.uuid4()),\\n        'timestamp': datetime.datetime.now().isoformat(),\\n        'random_value': random.randint(1, 1000),\\n        'description': 'Parent event message'\\n    }\\n\\ndef create_child_message(parent_id):\\n    \"\"\"Create a child message with its own ID and parent's ID.\"\"\"\\n    return {\\n        'id': str(uuid.uuid4()),\\n        'parent_id': parent_id,\\n        'timestamp': datetime.datetime.now().isoformat(),\\n        'random_value': random.randint(1, 1000),\\n        'description': 'Child event message'\\n    }\\n\\ndef produce_message_pair(producer, parent_topic, child_topic):\\n    \"\"\"Produce a pair of parent-child messages with random delay between them.\"\"\"\\n    # Create and send parent message\\n    parent_msg = create_parent_message()\\n    producer.send(parent_topic, parent_msg)\\n    print(f\"Produced to {parent_topic}: {parent_msg}\")\\n    \\n    # Random delay between parent and child messages (30 sec to 3 min)\\n    delay = random.uniform(30, 180)\\n    print(f\"Waiting {delay:.1f} seconds before sending child message...\")\\n    time.sleep(delay)\\n    \\n    # Create and send child message\\n    child_msg = create_child_message(parent_msg['id'])\\n    producer.send(child_topic, child_msg)\\n    print(f\"Produced to {child_topic}: {child_msg}\")\\n    \\n    # Random delay before next pair (30 sec to 5 min)\\n    if delay := random.uniform(30, 300):\\n        print(f\"Waiting {delay:.1f} seconds before next message pair...\")\\n        time.sleep(delay)\\n\\ndef main():\\n    # Parse command line arguments\\n    parser = argparse.ArgumentParser(description='Produce paired messages to Kafka topics')\\n    parser.add_argument('-b', '--bootstrap-servers', \\n                        default='localhost:9092',\\n                        help='Kafka bootstrap servers (default: localhost:9092)')\\n    parser.add_argument('-n', '--num-pairs',\\n                        type=int,\\n                        default=20,\\n                        help='Number of message pairs to produce (default: 20)')\\n    args = parser.parse_args()\\n    \\n    # Create Kafka producer\\n    producer = create_producer(args.bootstrap_servers)\\n    \\n    # Define topics\\n    parent_topic = 'ctm-parent-events'\\n    child_topic = 'ctm-children-events'\\n    \\n    try:\\n        print(f\"Starting to produce {args.num_pairs} message pairs...\")\\n        for i in range(args.num_pairs):\\n            print(f\"Producing pair {i+1} of {args.num_pairs}\")\\n            produce_message_pair(producer, parent_topic, child_topic)\\n            \\n        print(\"Finished producing all message pairs\")\\n            \\n    except KeyboardInterrupt:\\n        print(\"Production interrupted by user\")\\n    finally:\\n        producer.close()\\n        print(\"Producer closed\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n\\n",
      "SubApplication" : "producer",
      "FileName" : "kafka-producer.py",
      "Host" : "kafka",
      "Description" : "This produces event pairs. The number is determined by PARM1 which specifies the -n parameter.\\n\\nIf overriding during ordering, PARM1 has the syntax \"-n x\", where \"x\" is the number of event pairs to produce.",
      "RunAs" : "kafka",
      "Application" : "event orchestration",
      "Variables" : [ {
        "PARM1" : "-n 2"
      } ]
    },

    "flow00": {"Type":"Flow", "Sequence":[ 
      "jog-delay-producer",
      "jog-produce-kafka-events"
  ]}
  }
}